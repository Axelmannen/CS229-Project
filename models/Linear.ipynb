{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9828395,"sourceType":"datasetVersion","datasetId":6027555}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"URRRbyFF0j-Q"}},{"cell_type":"code","source":["!pip install optuna\n","!pip install joblib"],"metadata":{"id":"V6Y0c79yi6FD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import glob\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","\n","import matplotlib.pyplot as plt\n","\n","import optuna\n","import joblib"],"metadata":{"execution":{"iopub.status.busy":"2024-10-28T03:20:21.552201Z","iopub.execute_input":"2024-10-28T03:20:21.552626Z","iopub.status.idle":"2024-10-28T03:20:23.237657Z","shell.execute_reply.started":"2024-10-28T03:20:21.552588Z","shell.execute_reply":"2024-10-28T03:20:23.236478Z"},"trusted":true,"id":"ZpkpYSgJ0j-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if a GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")  # Use the first GPU\n","    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Using CPU\")"],"metadata":{"id":"XX3BEdvA6hXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"rgQY3sYi02LW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"BrpAuoCP0j-W"}},{"cell_type":"code","source":["class SpeedDataset(Dataset):\n","    def __init__(self, directory_paths, sequence_length):\n","        self.data = []\n","        self.labels = []\n","        self.timestamps = []\n","\n","        # Load data\n","        all_data = self.load_data_from_directories(directory_paths)\n","\n","        # print(f\"Number of files: {len(all_data)}\")\n","\n","        # Process data\n","        for data in all_data:\n","            magnitude_averages, labels, timestamps = self.create_magnitude_averages(data, sequence_length)\n","            self.data.extend(magnitude_averages)\n","            self.labels.extend(labels)\n","            self.timestamps.extend(timestamps)\n","\n","        self.data = torch.tensor(self.data, dtype=torch.float32)\n","        self.labels = torch.tensor(self.labels, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx], self.labels[idx]\n","\n","    def load_data_from_directories(self, directory_paths):\n","      \"\"\"\n","      Load and concatenate data from all CSV files in the specified directory.\n","      \"\"\"\n","      all_data = []\n","      for directory_path in directory_paths:\n","        for filename in os.listdir(directory_path):\n","            if filename.endswith(\".csv\"):\n","                file_path = os.path.join(directory_path, filename)\n","                data = pd.read_csv(file_path, header=None, names=['timestamp', 'x_acc', 'y_acc', 'z_acc', 'gps_speed'])\n","                all_data.append(data)\n","\n","      return all_data\n","\n","    def create_magnitude_averages(self, data, sequence_length):\n","      \"\"\"\n","      Create sequences and labels from the data.\n","      Each sequence consists of `sequence_length` rows, and the label is the GPS speed of the last row in each sequence.\n","      \"\"\"\n","      datapoints = []\n","      labels = []\n","      timestamps = []\n","\n","      for i in range(len(data) - sequence_length):\n","          sequence = np.abs(data.iloc[i:i+sequence_length][['x_acc', 'y_acc', 'z_acc']].values)\n","          average_magnitudes = np.mean(sequence, axis=0)\n","\n","          label = sum(data.iloc[i: i + sequence_length]['gps_speed']) / sequence_length\n","\n","          timestamp = data.iloc[i + sequence_length // 2]['timestamp']\n","\n","          datapoints.append(average_magnitudes)\n","          labels.append(label)\n","          timestamps.append(timestamp)\n","\n","      return np.array(datapoints), np.array(labels), np.array(timestamps)\n","\n","    # For testing working with non-overlapping sequences\n","    def create_magnitues_averages_non_overlapping(self, data, sequence_length):\n","      \"\"\"\n","      Create sequences and labels from the data.\n","      Each sequence consists of `sequence_length` rows, and the label is the GPS speed of the last row in each sequence.\n","      \"\"\"\n","      datapoints = []\n","      labels = []\n","      timestamps = []\n","      for i in range(len(data) // sequence_length):\n","          sequence = np.abs(data.iloc[i * sequence_length : (i + 1) * sequence_length][['x_acc', 'y_acc', 'z_acc']].values)\n","          average_magnitudes = np.mean(sequence, axis=0)\n","\n","          label = sum(data.iloc[i * sequence_length : (i + 1) * sequence_length]['gps_speed']) / sequence_length\n","          timestamp = data.iloc[i * sequence_length + sequence_length // 2]['timestamp']\n","\n","          datapoints.append(average_magnitudes)\n","          labels.append(label)\n","          timestamps.append(timestamp)\n","\n","      return np.array(datapoints), np.array(labels), np.array(timestamps)\n","\n","    def get_speed_distribution(self):\n","      \"\"\"\n","      Get the distribution of GPS speeds in the dataset.\n","      \"\"\"\n","\n","      print(\"Len labels\", len(self.labels))\n","      speed_counts = [0, 0, 0, 0, 0, 0]\n","      for label in self.labels:\n","          label = int(label)\n","          if label > 5:\n","              speed_counts[5] += 1\n","          else:\n","              speed_counts[label] += 1\n","\n","      # Bar chart\n","      plt.figure(figsize=(10, 4))\n","      plt.bar([f\"{i}-{i+1}\" if i < 5 else \"5+\" for i in range(len(speed_counts))], speed_counts)\n","      plt.xlabel('Speed (m/s)')\n","      plt.ylabel('Number of sequences')\n","      plt.show()\n"],"metadata":{"id":"RPzbD0iLISQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create training and testing datasets\n","train_dir = ['/content/drive/Shareddrives/CS229/Data/FinalSplits/train_axel/',\n","             '/content/drive/Shareddrives/CS229/Data/FinalSplits/train_jengchi/'\n","\n","\n","]\n","eval_dir = ['/content/drive/Shareddrives/CS229/Data/FinalSplits/eval_axel/']\n","test_dir = ['/content/drive/Shareddrives/CS229/Data/FinalSplits/test_axel/']\n","\n","sequence_length = 40\n","train_dataset = SpeedDataset(train_dir + eval_dir, sequence_length)"],"metadata":{"id":"bB8HFGp1wDw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_dataset.get_speed_distribution()"],"metadata":{"id":"odXt1h3SDroH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Loader"],"metadata":{"id":"S7GLWDe3HRpo"}},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"8RLXCvvBNNAU"}},{"cell_type":"code","source":["class LinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super(LinearRegressionModel, self).__init__()\n","        self.linear = nn.Linear(3, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"metadata":{"id":"NuDM5ZDBNVpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Optimize Hyperparameters\n"],"metadata":{"id":"SwGIbL2lipUi"}},{"cell_type":"code","source":["def objective(trial):\n","    # Define the hyperparameters to optimize\n","    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n","    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n","    # sequence_length = trial.suggest_int(\"sequence_length\", 10, 80, step=10)\n","\n","    sequence_length = 20\n","\n","    # Instantiate the model\n","    model = LinearRegressionModel().to(device)\n","\n","    # Loss function and optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","\n","    train_dataset = SpeedDataset(train_dir, sequence_length)\n","    eval_dataset = SpeedDataset(eval_dir, sequence_length)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    # Load training data\n","    X_train_tensor = train_dataset.data.to(device)\n","    y_train_tensor = train_dataset.labels.to(device)\n","\n","    # Load evaluation data\n","    X_eval_tensor = eval_dataset.data.to(device)\n","    y_eval_tensor = eval_dataset.labels.to(device)\n","\n","    # Training loop with early stopping\n","    max_epochs = 200\n","    patience = 20\n","    best_eval_loss = float('inf')\n","    epochs_without_improvement = 0\n","\n","    for epoch in range(max_epochs):\n","\n","        # Training\n","        model.train()\n","        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n","            X_tensor = X_batch.to(device)\n","            y_tensor = y_batch.to(device)\n","\n","            # Forward pass\n","            predictions = model(X_tensor)\n","            predictions = torch.squeeze(predictions)\n","            loss = criterion(predictions, y_tensor)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        with torch.no_grad():\n","            eval_predictions = model(X_eval_tensor)\n","            eval_predictions = torch.squeeze(eval_predictions)\n","            eval_loss = criterion(eval_predictions, y_eval_tensor)\n","\n","        # Check for improvement\n","        if eval_loss.item() < best_eval_loss:\n","            best_eval_loss = eval_loss.item()\n","            epochs_without_improvement = 0\n","        else:\n","            epochs_without_improvement += 1\n","\n","        # Early stopping check\n","        if epochs_without_improvement >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","        # Print progress\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch {epoch+1}/{max_epochs}, Eval Loss: {eval_loss.item():.4f}\")\n","\n","    return eval_loss.item()\n"],"metadata":{"id":"EdG5OE6qinz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=20)\n","study.set_user_attr(\"epochs\", epochs_for_best_trial[0])\n","print(\"Best Hyperparameters:\", study.best_params)"],"metadata":{"id":"0FDaTErrkP5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save study\n","joblib.dump(study, '/content/drive/Shareddrives/CS229/study_linear.pkl')"],"metadata":{"id":"MLP_-F89Q5bW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final training\n"],"metadata":{"id":"AjRY8XPP0j-Y"}},{"cell_type":"code","source":["# Load study\n","study = joblib.load('/content/drive/Shareddrives/CS229/study_linear.pkl')"],"metadata":{"id":"5cA71PZxRSxz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use best hyperparameters\n","# lr = study.best_params[\"learning_rate\"]\n","# batch_size = study.best_params[\"batch_size\"]\n","# sequence_length = study.best_params[\"sequence_length\"]\n","\n","# print(f\"Best learning rate: {lr}\")\n","# print(f\"Best batch size: {batch_size}\")\n","# print(f\"Best sequence length: {sequence_length}\")\n","\n","train_dir = [\n","    '/content/drive/Shareddrives/CS229/Data/FinalSplits/train_axel/',\n","    '/content/drive/Shareddrives/CS229/Data/FinalSplits/train_jengchi/'\n","]\n","eval_dir = ['/content/drive/Shareddrives/CS229/Data/FinalSplits/eval_axel/']\n","\n","sequence_length = 40\n","lr = 8.0e-05\n","batch_size = 128\n","\n","# Instantiate the model\n","model = LinearRegressionModel().to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","\n","train_dataset = SpeedDataset(train_dir, sequence_length)\n","eval_dataset = SpeedDataset(eval_dir, sequence_length)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Load training data\n","X_train_tensor = train_dataset.data.to(device)\n","y_train_tensor = train_dataset.labels.to(device)\n","\n","# Load evaluation data\n","X_eval_tensor = eval_dataset.data.to(device)\n","y_eval_tensor = eval_dataset.labels.to(device)\n","\n","# Training loop with early stopping\n","max_epochs = 500\n","patience = 20\n","best_eval_loss = float('inf')\n","epochs_without_improvement = 0\n","\n","train_losses = []\n","eval_losses = []\n","\n","for epoch in range(max_epochs):\n","\n","    # Training\n","    model.train()\n","    total_loss = 0.0\n","    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n","        X_tensor = X_batch.to(device)\n","        y_tensor = y_batch.to(device)\n","\n","        # Forward pass\n","        predictions = model(X_tensor)\n","        predictions = torch.squeeze(predictions)\n","        loss = criterion(predictions, y_tensor)\n","        total_loss += loss.item()\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    average_loss = total_loss / len(train_loader)\n","    train_losses.append(average_loss)\n","\n","    # Validation\n","    model.eval()\n","    with torch.no_grad():\n","        eval_predictions = model(X_eval_tensor)\n","        eval_predictions = torch.squeeze(eval_predictions)\n","        eval_loss = criterion(eval_predictions, y_eval_tensor)\n","        eval_losses.append(eval_loss.item())\n","\n","    # Check for improvement\n","    if eval_loss.item() < best_eval_loss:\n","        best_eval_loss = eval_loss.item()\n","        epochs_for_best_trial[0] = epoch + 1\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","\n","    # Early stopping check\n","    if epochs_without_improvement >= patience:\n","        print(f\"Early stopping at epoch {epoch+1}\")\n","        break\n","\n","    print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {average_loss:.4f}, Eval Loss: {eval_loss.item():.4f}\")\n","\n","\n"],"metadata":{"id":"qOj7ZknL6MbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot loss curves\n","plt.figure(figsize=(12, 6))\n","plt.plot(range(len(train_losses)), train_losses, label='Training Loss', color='blue')\n","plt.plot(range(len(eval_losses)), eval_losses, label='Validation Loss', color='red')\n","plt.ylim(0, 0.5)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Curves')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"L3T43GLQcUxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"knCuYuLN0j-Z"}},{"cell_type":"markdown","source":["### Calculate Metrics\n"],"metadata":{"id":"mStxamJxRBxM"}},{"cell_type":"code","source":["test_dir = ['/content/drive/Shareddrives/CS229/Data/FinalSplits/test_axel/']\n","# test_dir = ['/content/drive/Shareddrives/CS229/Data/FinalSplits/test_jengchi/']\n","\n","test_dataset = SpeedDataset(test_dir, sequence_length)\n","\n","\n","model.eval()\n","\n","predicted_train = model(train_dataset.data.to(device)).detach()\n","predicted_train = torch.squeeze(predicted_train).to(device)\n","\n","predicted_test = model(test_dataset.data.to(device)).detach()\n","predicted_test = torch.squeeze(predicted_test).to(device)\n","\n","mse_train = (np.square(predicted_train.to(\"cpu\") - train_dataset.labels.numpy())).mean(axis=0)\n","mse_test = (np.square(predicted_test.to(\"cpu\") - test_dataset.labels.numpy())).mean(axis=0)\n","\n","print(f\"MSE Train: {mse_train}\")\n","print(f\"MSE Test: {mse_test}\")"],"metadata":{"id":"B2pd36O1Q9Ey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot"],"metadata":{"id":"IT8d0bcKLSA0"}},{"cell_type":"code","source":["predicted_test = model(test_dataset.data.to(device)).detach().to(\"cpu\").numpy()\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(test_dataset.timestamps, test_dataset.labels, label='Ground Truth GPS Speed', color='blue')\n","plt.plot(test_dataset.timestamps, predicted_test, label='Predicted GPS Speed', color='red')\n","# plt.plot(test_dataset.timestamps, test_dataset.data.sum(dim=1) / 8, label='Combined magnitudes', color='green')\n","plt.xlabel('Timestamp (ms since start)')\n","plt.ylabel('Speed (m/s)')\n","plt.title('Ground Truth vs. Predicted GPS Speed')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"3xVzcXCMGYiW"},"execution_count":null,"outputs":[]}]}